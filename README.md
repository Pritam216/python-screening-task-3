# Python Screening Task 3 – Research Plan
## Research Plan

My approach to evaluating open-source models for student competence analysis in Python will focus on systematically identifying and testing models that can analyze code and generate meaningful feedback. I will begin by exploring open-source language models trained for code understanding, such as **CodeLlama-7B-Python**, and compare them with educational analytics frameworks like **Open Education Analytics**. To test their applicability, I will use a curated dataset of Python code samples representing beginner to advanced levels, including intentional mistakes (syntax errors, logical bugs, and common misconceptions such as variable scope confusion or faulty loops). The models will be evaluated on their ability to </br>
(1) analyze Python code structurally and semantically, </br>
(2) generate prompts that probe conceptual understanding rather than surface-level syntax, </br>
(3) identify reasoning gaps and misconceptions, and </br>
(4) provide feedback that supports deeper learning without directly giving away solutions. Validation will include benchmarking against datasets such as HumanEval-Python, as well as expert review of model-generated prompts using **Bloom’s taxonomy** as a framework. </br>

A suitable model for high-level competence analysis must demonstrate deep code comprehension (beyond syntax checking), educational awareness of how students learn and where they struggle, contextual reasoning to explain why mistakes occur, and adaptive communication for learners of different levels. To test whether a model generates meaningful prompts, I would evaluate their cognitive depth (using Bloom’s taxonomy), ability to target misconceptions, progression in difficulty, and relevance as judged by Python educators. In terms of trade-offs, larger models may provide higher accuracy but require more computational resources, while smaller models are more affordable and interpretable but may miss subtle errors. I selected **CodeLlama-7B-Python** as the evaluation target because it balances accuracy, interpretability, and accessibility. Its strengths include Python-focused training, open-source flexibility, and reasonable computational needs, while its limitations include less explicit pedagogical training and potential inconsistency compared to larger models. These limitations can be mitigated with fine-tuning and integration with educational frameworks like **DeepEval** for consistency checks.
